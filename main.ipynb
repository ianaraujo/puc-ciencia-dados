{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predição de Fraudes \"Card Not Present\" (IEEE-CIS Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O problema escolhido para realização do projeto foi de fraudes em transações sem cartão presencial, conhecidas como **\"Card Not Present Fraud\"**. Os dados foram preparados e disponibilizados pela IEEE Computational Intelligence Society e lançados durante uma competição [IEEE-CIS Fraud Detection](https://www.kaggle.com/competitions/ieee-fraud-detection/overview) no Kaggle.\n",
    "\n",
    "O que torna esse projeto mais interessante é que os dados são de **transações reais** foram fornecidos pela [Vesta Corporation](https://vesta.io/). A Vesta é uma empresa especializada em soluções de proteção contra fraudes e processamento de pagamentos para transações móveis e online. A empresa utiliza modelos avançados de *machine learning* para analisar mais de US$ 4 bilhões em transações anualmente, fornecendo serviços que permitem a aprovação de vendas em milissegundos e o processamento de pagamentos em mais de 40 países.\n",
    "\n",
    "O conjunto de dados possui cerca de 600.000 registros e contém mais de 430 características. Portanto, além do desafio de modelagem para predição de fraudes, é preciso considerar o grande volume de dados e a alta dimensionalidade. Por serem transações reais, um grande número de *features* foram anonimizadas, garantindo a privacidade dos clientes, portanto, não é possível saber qual o conteúdo real de todas as variáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição do problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a seguinte situação. Você compra um café da manhã na padaria e se dirige ao caixa para pagar. Como é muito comum nos tempos atuais, você utiliza seu telefone para pagar, usando um cartão digital por aproximação, mas sua compra é negada. Ou você decide aproveitar as promoções de fim de ano em determinada plataforma de e-commerce, preenche as informações do seu cartão, mas sua compra não pode ser efetuada.\n",
    "\n",
    "Embora você tenha certeza que possui dinheiro suficiente para realizar a compra, por algum motivo sua transação não é efetuada. Isso acontece por que, todos os anos, mais de 30 bilhões de dólares são movimentados em transações fraudulentas de cartões de crédito e as empresas envolvidas nesses serviços, como bancos, instituições de pagamento e prestadoras de serviço, investem pesado em **sistemas de prevenção de fraude**. Apesar de nem sempre serem acertivos, como no caso descrito acima quando houve uma falso positivo, esses sistema podem evitar uma grande dor de cabeça para os clientes e para essas instituições.\n",
    "\n",
    "A fraude em transações \"Card Not Present\" (CNP) ocorre quando compras são realizadas sem a presença física do cartão. Com o avanço da tecnologia e a popularização das compras pela internet, esse tipo de transação tornou-se predominante. No Brasil, por exemplo, 61% dos consumidores preferem comprar online em vez de em lojas físicas, e 78% realizam pelo menos uma compra mensal pela internet. Nessas compras, os consumidores inserem os dados do cartão, como número, data de validade e código de segurança, ou utilizam carteiras digitais em dispositivos móveis, facilitando as transações, mas também aumentando os riscos de fraude.\n",
    "\n",
    "Considerando esse cenário, o objetivo do trabalho é criar um **modelo de classificação** capaz de estimar a probabilidade das trações serem verdadeiras ou fraudes. Em paralelo, também discutir conceitos importantes desse tipo de problema, como classes desbalanceadas, redução de dimensionalidade e uso adequado de métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# sklearn models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como discutido, neste problema vamos criar um classificador para prever a probabilidade de uma transação ser fraudulenta, indicada pelo alvo binário `isFraud`. Os dados de treinamento do Kaggle estão divididos em dois arquivos, `train_identity.csv` e `train_transaction.csv`, que são unidos pela chave `TransactionID`.\n",
    "\n",
    "Por ser uma competição, o Kaggle não permite baixar os arquivos sem fazer login, por isso, não foi possível ler os dados através da URL. Foi necessário baixar os arquivos, que podem ser encontrados na pasta `/data` e ler localmente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity = pd.read_csv('../data/train_identity.csv')\n",
    "train_identity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns = train_identity.columns.difference(['TransactionID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por causa do grande volume de *features* nos dados de transações (394), vamos ler os dados em *chunks* ou blocos de 100.000 registros, o que otimiza a alocação de memória da máquina. Após concatenar os blocos em um único DataFrame, podemos deletar esses blocos para economizar recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_chunks = pd.read_csv('../data/train_transaction.csv', chunksize=10 ** 5)\n",
    "train_transaction = pd.concat(transaction_chunks)\n",
    "\n",
    "train_transaction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del transaction_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated = np.sum(train_transaction['TransactionID'].isin(train_identity['TransactionID'].unique()))\n",
    "total_transaction = train_transaction.shape[0]\n",
    "\n",
    "pct_records = np.divide(associated, total_transaction) * 100\n",
    "\n",
    "print(f'{pct_records:.2f}% do registro em Transaction ({total_transaction}) possuem registros de Identity associados.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### isFraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction['isFraud'].value_counts(normalize=True).round(4) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TransactionAmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False)\n",
    "\n",
    "ax1.set_title('Transaction Amounts <= 1000')\n",
    "ax2.set_title('Transaction Amounts (Log Scale)')\n",
    "\n",
    "sns.histplot(train_transaction.loc[train_transaction['TransactionAmt'] <= 1000], x='TransactionAmt', bins=100)\n",
    "sns.histplot(train_transaction, x='TransactionAmt', bins=100, log_scale=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True)\n",
    "ax1.set_title('isFraud = 0')\n",
    "ax2.set_title('isFraud = 1')\n",
    "\n",
    "sns.histplot(train_transaction.loc[train_transaction['isFraud'] == 0], x='TransactionAmt', color='tab:green', bins=100, log_scale=True, ax=ax1)\n",
    "sns.histplot(train_transaction.loc[train_transaction['isFraud'] == 1], x='TransactionAmt', color='tab:red', bins=100, log_scale=True, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = train_transaction['isFraud'] == 1\n",
    "\n",
    "mean_fraud = np.mean(train_transaction.loc[fraud]['TransactionAmt'])\n",
    "mean_not_fraud = np.mean(train_transaction.loc[not fraud]['TransactionAmt'])\n",
    "\n",
    "print(f\"Valor médio por transações isFraud == 1 é {mean_fraud:.2f}\")\n",
    "print(f\"Valor médio por transações isFraud == 0 é {mean_not_fraud:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Card Issuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction['card4'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_card_fraud = train_transaction.groupby(['card4', 'isFraud'])['TransactionAmt'].sum() / train_transaction.groupby(['card4'])['TransactionAmt'].sum()\n",
    "pct_card_fraud = (pct_card_fraud * 100).unstack(level=0).reset_index(drop=True)\n",
    "\n",
    "pct_card_fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction['card6'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeviceType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity['DeviceInfo'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity['DeviceType'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E-mail Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction['R_emaildomain'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_email_fraud = train_transaction.groupby(['R_emaildomain', 'isFraud'])['TransactionAmt'].sum() / train_transaction.groupby(['R_emaildomain'])['TransactionAmt'].sum()\n",
    "pct_email_fraud = (pct_email_fraud * 100).unstack('isFraud').dropna()\n",
    "\n",
    "pct_email_fraud.sort_values(by=1, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de Componentes Principais (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_chunks = []\n",
    "transaction_chunks = pd.read_csv('../data/train_transaction.csv', chunksize=10 ** 5)\n",
    "\n",
    "for chunk in transaction_chunks:\n",
    "    new_chunk = chunk.merge(train_identity, on='TransactionID', how='left')\n",
    "    merged_chunks.append(new_chunk)\n",
    "\n",
    "train = pd.concat(merged_chunks)\n",
    "train = train.groupby('isFraud', group_keys=False)[train.columns].apply(lambda x: x.sample(frac=0.1))\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_identity, transaction_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.loc[:, train.columns.str.startswith('V')]\n",
    "\n",
    "print(f'Número de features escolhidas: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train_transformed = numeric_transformer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train_transformed)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"n_components\")\n",
    "plt.ylabel(\"explained_variance_ratio\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variance_ratio = 0.95\n",
    "d = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= target_variance_ratio) + 1\n",
    "\n",
    "print(f\"Número de dimensões para manter {target_variance_ratio*100}% da variância: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hasIdentity'] = (train[identity_columns].isna().all(axis=1) == False).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adicionar outpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = 'TransactionAmt'\n",
    "selected_cols = ['card1', 'card4', 'P_emaildomain', 'R_emaildomain', 'addr1']\n",
    "\n",
    "for col in selected_cols:\n",
    "    train[f'amount_mean_{col}'] = train[amount] / train.groupby([col])[amount].transform('mean')\n",
    "    train[f'amount_std_{col}'] = train[amount] / train.groupby([col])[amount].transform('std')\n",
    "\n",
    "train['TransactionAmtLog'] = np.log(train[amount])\n",
    "train['TransactionAmtCents'] = (train[amount] - np.floor(train[amount])).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adicionar oupyut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.strptime('2022-01-01', '%Y-%m-%d')\n",
    "\n",
    "train['Date'] = train['TransactionDT'].apply(lambda dt: start_date + timedelta(seconds=dt))\n",
    "\n",
    "train['Weekday'] = train['Date'].dt.dayofweek\n",
    "train['Day'] = train['Date'].dt.day\n",
    "train['Hour'] = train['Date'].dt.hour\n",
    "\n",
    "train = train.drop(columns=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['TransactionDT', 'Weekday', 'Day', 'Hour']].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Features (Transaction)\n",
    "\n",
    "- ProductCD\n",
    "- emaildomain\n",
    "- card1 - card6\n",
    "- addr1, addr2\n",
    "- P_emaildomain\n",
    "- R_emaildomain\n",
    "- M1 - M9\n",
    "\n",
    "Categorical Features (Identity)\n",
    "\n",
    "- DeviceType\n",
    "- DeviceInfo\n",
    "- id_12 - id_38\n",
    "\n",
    "Categorical Created Features\n",
    "\n",
    "- hasIdentity\n",
    "- Weekday\n",
    "- Day\n",
    "- Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_FEATURES = [\n",
    "    *[f'card{i}' for i in range(1, 7)],\n",
    "    *[f'M{i}' for i in range(1, 10)],\n",
    "    *[f'id_{i}' for i in range(12, 39)],\n",
    "    \"ProductCD\",\n",
    "    \"emaildomain\",\n",
    "    \"addr1\", \n",
    "    \"addr2\",\n",
    "    \"P_emaildomain\",\n",
    "    \"R_emaildomain\",\n",
    "    \"DeviceType\",\n",
    "    \"DeviceInfo\",\n",
    "    \"hasIdentity\",\n",
    "    \"Weekday\",\n",
    "    \"Day\",\n",
    "    \"Hour\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['isFraud'])\n",
    "y = train['isFraud']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=23)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.replace([np.inf, -np.inf], -999)\n",
    "X_test = X_test.replace([np.inf, -np.inf], -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pca_features \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns[X_train\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      2\u001b[0m cat_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(CAT_FEATURES \u001b[38;5;241m+\u001b[39m X_train\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      3\u001b[0m num_features \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cat_features \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pca_features]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "pca_features = X_train.columns[X_train.columns.str.startswith('V')]\n",
    "cat_features = np.unique(CAT_FEATURES + X_train.select_dtypes(include=['object']).columns.tolist())\n",
    "num_features = [col for col in X_train.columns if col not in cat_features and col not in pca_features]\n",
    "\n",
    "print(np.sum(list(map(len, [pca_features, cat_features, num_features]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('to_string', FunctionTransformer(lambda X: X.astype(str))),\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "pca_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=90))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, cat_features),\n",
    "        ('num', numeric_transformer, num_features),\n",
    "        ('pca', pca_transformer, pca_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[1;32m      2\u001b[0m X_test_transformed \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = decision_tree.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = decision_tree.score(X_train_transformed, y_train)\n",
    "test_score = decision_tree.score(X_test_transformed, y_test)\n",
    "\n",
    "print(\"Train score: {}\".format(train_score))\n",
    "print(\"Test score: {}\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ROC AUC: {roc_auc_score(y_test, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1 Score: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "k_folds = 5\n",
    "cv = StratifiedKFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "models = {\n",
    "    'LGBM': LGBMClassifier(verbose=-1, random_state=23),\n",
    "    'RF': RandomForestClassifier(random_state=23),\n",
    "    'BC': BaggingClassifier(random_state=23),\n",
    "    'DT': DecisionTreeClassifier(random_state=23),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_transformed, y_train, cv=cv, scoring='roc_auc')\n",
    "    results[name] = scores\n",
    "\n",
    "    print(f'{name}: {scores.mean()} ({scores.std()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure() \n",
    "fig.suptitle('ROC AUC') \n",
    "\n",
    "ax = fig.add_subplot(111) \n",
    "ax.set_xticklabels(results.keys()) \n",
    "\n",
    "plt.boxplot(results.values()) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "\n",
    "X_train_res, y_train_res = undersampler.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "scores = cross_val_score(RandomForestClassifier(random_state=42), X_train_res, y_train_res, cv=cv, scoring='roc_auc')\n",
    "print(f'UNDER_RF: {scores.mean()} ({scores.std()})')\n",
    "\n",
    "results['UNDER_RF'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_boost = RUSBoostClassifier(random_state=42)\n",
    "\n",
    "scores = cross_val_score(rus_boost, X_train_transformed, y_train, cv=cv, scoring='roc_auc')\n",
    "print(f'RUSB: {scores.mean()} ({scores.std()})')\n",
    "\n",
    "results['RUSB'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "scores = cross_val_score(LGBMClassifier(random_state=42, verbose=-1), X_train_res, y_train_res, cv=cv, scoring='roc_auc')\n",
    "print(f'SMOTE_LGBM: {scores.mean()} ({scores.std()})')\n",
    "\n",
    "results['SMOTE_LGBM'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = dict(sorted(results.items(), key=lambda x: np.median(x[1]), reverse=True))\n",
    "\n",
    "fig = plt.figure() \n",
    "fig.suptitle('ROC AUC') \n",
    "\n",
    "ax = fig.add_subplot(111) \n",
    "ax.set_xticklabels(sorted_results.keys(), rotation=45) \n",
    "\n",
    "plt.boxplot(sorted_results.values()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "\n",
    "X_train_res, y_train_res = undersampler.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred = clf.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ROC AUC: {roc_auc_score(y_test, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1 Score: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred = clf.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ROC AUC: {roc_auc_score(y_test, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1 Score: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted Weights Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ROC AUC: {roc_auc_score(y_test, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1 Score: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100], # quanto maior melhor\n",
    "    'max_depth': [50, 60, 75]\n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, n_jobs=8)\n",
    "cv = StratifiedKFold(n_splits=k_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(clf, param_grid, cv=cv, scoring='roc_auc')\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise de threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(clf, X_train_res, y_train_res, cv=cv, method='predict_proba')\n",
    "\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshold = precision_recall_curve(y_train_res, y_scores[:, 1])\n",
    "\n",
    "plt.plot(threshold, precision[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(threshold, recall[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Precision/Recall\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Target threshold to obtain 90% precision is {threshold[np.argmax(precision >= 0.90)]}')\n",
    "print(f'Target threshold to obtain 90% precision is {threshold[np.argmin(recall >= 0.90)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall, precision, linewidth=2, label=\"Precision/Recall curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_threshold = 0.51\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_scores = clf.predict_proba(X_test_transformed)[:, 1]\n",
    "y_pred = (y_scores >= target_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ROC AUC: {roc_auc_score(y_test, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1 Score: {f1_score(y_test, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
